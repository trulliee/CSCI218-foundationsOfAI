{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHrnLjBy7WMU"
      },
      "source": [
        "# **Project Topic: Scene Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkJS0L2kqUxw"
      },
      "source": [
        "In this research, we investigate the performance of three distinct pretrained models for scene recognition. Our objective is to identify the most effective model by evaluaating their acuracy and robustness. Through systematic experimentation with various training and testing methodologies, we aim to determine the optimal approach for achieving the highest recognition performance. We choose this topic because scene recognition is not only a fasinating area of computer vision but also a challenging one that pushes us to think creatively and learn beyond conventional boundaries. By exploring different models and techniques, we hope to deepen our understanding of the field while contributing meaningful insights to the problem of scene recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-MoBnqUkoil",
        "outputId": "0805779d-3437-4f06-a516-223b5130d1c3"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import optuna\n",
        "import shutil\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWlEZpo97hWR"
      },
      "source": [
        "## **Step 1: Upload dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyXUW3YuHRFJ",
        "outputId": "f9e5b2d2-26dd-41e0-9eb1-e66d0b00454b"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"itsahmad/indoor-scenes-cvpr-2019\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX8hVn4Blr4I"
      },
      "source": [
        "After uploading the dataset, our next step is to analyze and understand its structure. Using the provided API call, we aim to identify the files and folders within the dataset, along with their respective names. This process will help us determine the number of image cateogries we need to correctly identify, as well as the quantity of images available per category. By gaining this insight, we can better prepare for the subsequent stages of training and testing our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRB_em-hzbhM"
      },
      "source": [
        "## **Step 2: Understanding the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOhELpTuUiaA",
        "outputId": "3276be49-594e-4036-8376-79196f3baacf"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(path))  # List all files and folders in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7viv-IN5vvax",
        "outputId": "53c1691f-5c3f-40ec-afd7-e20289ed1c66"
      },
      "outputs": [],
      "source": [
        "# Identifying what is a file or folder in the dataset\n",
        "for item in os.listdir(path):\n",
        "    item_path = os.path.join(path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        print(f\"{item} is a FOLDER\")\n",
        "    else:\n",
        "        print(f\"{item} is a FILE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DStfpPGqwYUF",
        "outputId": "4a62b26c-a772-48e0-be91-7fd057ce9184"
      },
      "outputs": [],
      "source": [
        "image_dir = os.path.join(path, \"indoorCVPR_09\")  # Adjust if images are elsewhere\n",
        "print(os.listdir(image_dir))  # List category folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyN9-Kxjwca-",
        "outputId": "0aa247ce-8de2-4909-c8e1-82156f99daf4"
      },
      "outputs": [],
      "source": [
        "# Checking number of categories\n",
        "category_counts = {}\n",
        "\n",
        "for category in os.listdir(image_dir):\n",
        "    category_path = os.path.join(image_dir, category)\n",
        "\n",
        "    if os.path.isdir(category_path):  # Ensure it's a folder\n",
        "        num_images = len(os.listdir(category_path))  # Count images\n",
        "        category_counts[category] = num_images\n",
        "\n",
        "# Print results\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"{category}: {count} categories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0p8WkxXy9O1"
      },
      "source": [
        "The dataset provides TrainImages.txt and TestImages.txt, which are a list of images, roughly 80 images per category. This represents a subset of the full dataset used for training purposes. Let's preview them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I4Xa93Rxybe",
        "outputId": "c6fcbf02-d7e6-4e03-dad3-92c947b7a0ef"
      },
      "outputs": [],
      "source": [
        "train_file = os.path.join(path, \"TrainImages.txt\")\n",
        "\n",
        "with open(train_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Print the first 5 lines to check the structure\n",
        "for line in lines[:5]:\n",
        "    print(line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5CNJczSK_"
      },
      "source": [
        "Now, let's extract categories and count the number of images per category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS1IpAzayB0W",
        "outputId": "b197378b-803b-45eb-facc-3b9ebbd69be5"
      },
      "outputs": [],
      "source": [
        "# Initialize dictionary\n",
        "category_counts = defaultdict(int)\n",
        "\n",
        "# Read file and count occurrences of each category\n",
        "with open(train_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        category = line.strip().split('/')[0]  # Extract category name\n",
        "        category_counts[category] += 1\n",
        "\n",
        "# Sort categories by image count\n",
        "sorted_counts = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print results\n",
        "for category, count in sorted_counts:\n",
        "    print(f\"{category}: {count} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckCkgfVczVkF"
      },
      "source": [
        "And finally, plot the bar chart for a better visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "8-YeY7UAyF39",
        "outputId": "d0a22de6-6871-4aeb-f307-c7699eb9b69e"
      },
      "outputs": [],
      "source": [
        "# Extract sorted category names and counts\n",
        "categories, counts = zip(*sorted_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(categories, counts, color=\"skyblue\")\n",
        "plt.xlabel(\"Indoor Scene Categories\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Number of Images per Indoor Scene Category\")\n",
        "plt.xticks(rotation=90)  # Rotate labels for readability\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Show the chart\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rXy11NH0qiM"
      },
      "source": [
        "So there are 67 categories in total that we need to have our model correctly identify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuwqz9iDkoip"
      },
      "source": [
        "## **Step 3: Defining Model Training Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xnkc4sxAR39"
      },
      "source": [
        "Here we do a deep learning model using PyTorch, optimizing it with Adam while incorporating a learning rate scheduler and early stopping to improve validation accuracy. It tracks training and validation accuracy over epochs, stopping early if no improvement is observed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxI0wFMdXrV9"
      },
      "outputs": [],
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model, train_loader, val_loader, best_params, num_epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use best hyperparameters\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=best_params['learning_rate'],\n",
        "        weight_decay=best_params['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Add learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.1, patience=2, verbose=True\n",
        "    )\n",
        "\n",
        "    # Add early stopping\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_acc_history, val_acc_history = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        train_acc_history.append(train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_correct, val_total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_acc_history.append(val_accuracy)\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            patience_counter = 0  # Reset counter if improvement occurs\n",
        "        else:\n",
        "            patience_counter += 1  # Increment counter if no improvement\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss:.4f} - Train Acc: {train_accuracy:.4%} - Val Acc: {val_accuracy:.4%}\")\n",
        "\n",
        "        # Stop training if patience threshold is reached\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return model, train_acc_history, val_acc_history, training_time, best_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mns0lfmqAsMI"
      },
      "source": [
        "Then we evaluate the trained model on the test dataset by computing predictions, comparing them to true labels and calculating overall test accuracy. It returns the true labels, predicted labels and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ9zHkaqXuKf"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    test_acc = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return y_true, y_pred, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N8CeWEtBA2s"
      },
      "source": [
        "And then we plot a confusion matrix to visualize the model's classification perfomance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7wvIMLnXw41"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JDc_yAi7mtB"
      },
      "source": [
        "## **Step 4: Data Processing and Splitting the data into training, validation and test sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiDnBnHRBXFE"
      },
      "source": [
        "In this step, we organize an image dataset by splitting it into training, validation and test sets while preserving category structures. It reads image file lists, shuffles and partitions them based on predefined ratios, creates necessary dictionaries and copies images to their respective folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_vVKTgIVS0W",
        "outputId": "d40aabae-e6ed-4336-bb50-425664973e57"
      },
      "outputs": [],
      "source": [
        "# Define file paths\n",
        "train_file = os.path.join(path, \"TrainImages.txt\")\n",
        "test_file = os.path.join(path, \"TestImages.txt\")\n",
        "input_folder = os.path.join(path, \"indoorCVPR_09/Images\")\n",
        "output_folder = \"truncated_dataset\"\n",
        "\n",
        "# Create output directories\n",
        "train_output = os.path.join(output_folder, \"train\")\n",
        "val_output = os.path.join(output_folder, \"val\")\n",
        "test_output = os.path.join(output_folder, \"test\")\n",
        "os.makedirs(train_output, exist_ok=True)\n",
        "os.makedirs(val_output, exist_ok=True)\n",
        "os.makedirs(test_output, exist_ok=True)\n",
        "\n",
        "def organize_dataset(input_folder, train_file, test_file, output_folder, val_split=0.2):\n",
        "\n",
        "    # Read train and test files\n",
        "    with open(train_file, 'r') as f:\n",
        "        train_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    with open(test_file, 'r') as f:\n",
        "        test_images = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # Split train images into train and validation\n",
        "    train_imgs, val_imgs = train_test_split(train_images, test_size=val_split, random_state=42)\n",
        "\n",
        "    # Function to copy images\n",
        "    def copy_images(image_list, target_dir):\n",
        "        for img_path in image_list:\n",
        "            category = img_path.split('/')[0]\n",
        "            filename = img_path.split('/')[-1]\n",
        "\n",
        "            # Create category directory if it doesn't exist\n",
        "            category_dir = os.path.join(target_dir, category)\n",
        "            os.makedirs(category_dir, exist_ok=True)\n",
        "\n",
        "            # Copy image\n",
        "            src_path = os.path.join(input_folder, img_path)\n",
        "            dst_path = os.path.join(category_dir, filename)\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    # Copy images to respective directories\n",
        "    print(\"Copying training images...\")\n",
        "    copy_images(train_imgs, train_output)\n",
        "\n",
        "    print(\"Copying validation images...\")\n",
        "    copy_images(val_imgs, val_output)\n",
        "\n",
        "    print(\"Copying test images...\")\n",
        "    copy_images(test_images, test_output)\n",
        "\n",
        "    print(\"Dataset organization complete!\")\n",
        "\n",
        "# Use the function\n",
        "input_folder = os.path.join(path, \"indoorCVPR_09/Images\")\n",
        "train_file = os.path.join(path, \"TrainImages.txt\")\n",
        "test_file = os.path.join(path, \"TestImages.txt\")\n",
        "output_folder = \"dataset\"\n",
        "\n",
        "organize_dataset(input_folder, train_file, test_file, output_folder)\n",
        "\n",
        "def organize_full_dataset(input_folder, output_folder=\"full_dataset\", train_split=0.7, val_split=0.2):\n",
        "    \"\"\"\n",
        "    Organizes the complete dataset into train/val/test splits while maintaining category structure.\n",
        "    Args:\n",
        "        input_folder: Path to input folder containing category subfolders\n",
        "        output_folder: Path to output folder\n",
        "        train_split: Percentage of data for training (default: 0.7)\n",
        "        val_split: Percentage of data for validation (default: 0.2)\n",
        "        (test_split will be the remaining: 0.1)\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    train_dir = os.path.join(output_folder, \"train\")\n",
        "    val_dir = os.path.join(output_folder, \"val\")\n",
        "    test_dir = os.path.join(output_folder, \"test\")\n",
        "\n",
        "    for dir_path in [train_dir, val_dir, test_dir]:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    # Get all categories\n",
        "    categories = [d for d in os.listdir(input_folder)\n",
        "                 if os.path.isdir(os.path.join(input_folder, d))]\n",
        "\n",
        "    print(f\"Found {len(categories)} categories\")\n",
        "\n",
        "    # Process each category\n",
        "    for category in categories:\n",
        "        print(f\"Processing category: {category}\")\n",
        "\n",
        "        # Create category directories in each split\n",
        "        category_dirs = {\n",
        "            'train': os.path.join(train_dir, category),\n",
        "            'val': os.path.join(val_dir, category),\n",
        "            'test': os.path.join(test_dir, category)\n",
        "        }\n",
        "\n",
        "        for dir_path in category_dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # Get all images in the category\n",
        "        category_input_dir = os.path.join(input_folder, category)\n",
        "        images = [f for f in os.listdir(category_input_dir)\n",
        "                 if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        # Shuffle images\n",
        "        random.seed(42)  # for reproducibility\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Calculate split indices\n",
        "        n_images = len(images)\n",
        "        n_train = int(n_images * train_split)\n",
        "        n_val = int(n_images * val_split)\n",
        "\n",
        "        # Split images\n",
        "        train_images = images[:n_train]\n",
        "        val_images = images[n_train:n_train + n_val]\n",
        "        test_images = images[n_train + n_val:]\n",
        "\n",
        "        # Copy images to respective splits\n",
        "        for split_name, split_images in [\n",
        "            ('train', train_images),\n",
        "            ('val', val_images),\n",
        "            ('test', test_images)\n",
        "        ]:\n",
        "            for image in split_images:\n",
        "                src_path = os.path.join(category_input_dir, image)\n",
        "                dst_path = os.path.join(category_dirs[split_name], image)\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    print(\"Full dataset organization complete!\")\n",
        "    print(f\"Split ratios - Train: {train_split:.1%}, Val: {val_split:.1%}, Test: {(1-train_split-val_split):.1%}\")\n",
        "\n",
        "# Use the function to create the full dataset with splits\n",
        "input_folder = os.path.join(path, \"indoorCVPR_09/Images\")\n",
        "organize_full_dataset(input_folder, output_folder=\"full_dataset\", train_split=0.7, val_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmjskdFxCAfA"
      },
      "source": [
        "This is where we define image transformations for training and evaluation, including resizing, normalization, and data augmentation for training (random flipping, rotation, and cropping). After that, we load the image datasets from specified directories using these transformations and create DataLoader objects for batch processing. Lastly, we retrieve class names from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsvnGeHSXpKT"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=\"truncated_dataset/train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=\"truncated_dataset/val\", transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"truncated_dataset/test\", transform=eval_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Get class names\n",
        "class_names = train_dataset.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRNcpz26v323"
      },
      "source": [
        "## **Step 4.5 Hyperparemeter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL14_5EICtx2"
      },
      "source": [
        "Here we define an Optuna objective function for hyperparameter tuning for the models. It optimizes learning rate, weight decay, dropout rate, batch size, updates DataLoaders accordingly, and initializes a Timm-based pretrained model with dynamic dropout. The models are trained for 5 epochs, using Adam optimizer, learning rate scheduling and early stopping based on validation accuracy. During training, Optuna tracks performance, and trials that perform poorly are pruned early to speed up the search. After each trial, CUDA memory is cleared if available to prevent memory issues. The function returns the best validation accuracy for hyperparameter optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mABOrC4v323"
      },
      "outputs": [],
      "source": [
        "def create_objective(model_architecture):\n",
        "    def objective(trial):\n",
        "        # Hyperparameters to tune\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "        batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
        "\n",
        "        # Update dataloaders with new batch size\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Create model with updated dropout\n",
        "        model = timm.create_model(\n",
        "            model_architecture,\n",
        "            pretrained=True,\n",
        "            num_classes=len(class_names),\n",
        "            drop_rate=dropout_rate,\n",
        "            drop_path_rate=dropout_rate/2\n",
        "        )\n",
        "\n",
        "        model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.1, patience=2, verbose=True\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "        patience = 3\n",
        "\n",
        "        for epoch in range(5):  # Reduced epochs for faster optimization\n",
        "            model.train()\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_correct, val_total = 0, 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "            val_accuracy = val_correct / val_total\n",
        "            scheduler.step(val_accuracy)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "            trial.report(val_accuracy, epoch)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return best_val_acc\n",
        "\n",
        "    return objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eIC-nykoiq"
      },
      "source": [
        "## **Step 5: Vision Transformer Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfjO4XeEP02"
      },
      "source": [
        "Our first model is Vision Transformers (ViT) -- A transformer-based model for image recognition that processes images as sequences of patches, leveraging self-attention for feature extraction instead of convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HX3l8YybM_y",
        "outputId": "c1beb270-fb83-4d7e-a969-7b9290916716"
      },
      "outputs": [],
      "source": [
        "# For ViT\n",
        "print(\"Optimizing Vision Transformer hyperparameters...\")\n",
        "vit_study = optuna.create_study(direction='maximize')\n",
        "vit_objective = create_objective('vit_base_patch16_224')\n",
        "vit_study.optimize(vit_objective, n_trials=10)\n",
        "vit_best_params = vit_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e_Xpn_BqX0Q2",
        "outputId": "2fc641ab-96b5-4b96-9c9f-1b6f3d312a71"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=vit_best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=vit_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "vit_model = timm.create_model(\n",
        "    \"vit_base_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=len(class_names),\n",
        "    drop_rate=vit_best_params['dropout_rate'],\n",
        "    drop_path_rate=vit_best_params['dropout_rate']/2\n",
        ")\n",
        "\n",
        "vit_model, vit_train_acc_history, vit_val_acc_history, vit_time, vit_val_acc = train_model(vit_model, train_loader, val_loader, vit_best_params, num_epochs=10)\n",
        "\n",
        "# Evaluate\n",
        "y_true, y_pred, vit_test_acc = evaluate_model(vit_model, test_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Vision Transformer (ViT):\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, \"Vision Transformer (ViT)\")\n",
        "\n",
        "print(f\"Training Time for Vision Transformer (ViT): {vit_time:.2f} seconds\\n\")\n",
        "print(f\"Accuracy for Vision Transformer (ViT): {vit_test_acc:.2f}\\n\")\n",
        "\n",
        "# Free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYA08resHU7P"
      },
      "source": [
        "From the results of the Vision Transformer (ViT) model, we observe that the model achieved a solid test accuracy of 83.73% and a reasonable treaining time of 687.98 seconds, suggesting a balanced performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Top 3 best performing categories:\n",
        "1. Bowling\n",
        "2. Casino\n",
        "3. Florist\n",
        "\n",
        "\n",
        "---\n",
        "Top 3 worst performing categories:\n",
        "1. Deli\n",
        "2. Toystore\n",
        "3. Children's Room\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaSlANkikoiq"
      },
      "source": [
        "## **Step 6: Convnext Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glN9bwhaEp96"
      },
      "source": [
        "Second model is ConvNeXt -- A modernized CNN architecture inspired by ViTs, desgined with ResNet-like structures but optimized with dept-wise convolutions and LayerNorm for improved efficiency and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "af5780f2665e407490a9a20865e83e37",
            "97e02590e4b7424c95d4e03bd7e1c33f",
            "5392699f66714d2591167c25585a1202",
            "9645acf8eba445339d3e21d227e90258",
            "1662b88f70d14b3a8d47116f8af93e0f",
            "bac0b9b803634406bfcde7db785daaf8",
            "31a47e6965bc4ecebb93cb7010a86254",
            "bf547ce8e4e44042890226fa3be54387",
            "fa786bbf617c4122b3a029a57083201a",
            "1c2ebddeec0942b6b0b304815902d8a5",
            "af3a2d5a9fe94f43a8527af8b6df2221"
          ]
        },
        "id": "gR8c2j0zbM_z",
        "outputId": "99cd4055-8ec1-40cd-a43c-16423e21a5c6"
      },
      "outputs": [],
      "source": [
        "# For ConvNeXt\n",
        "print(\"Optimizing ConvNeXt hyperparameters...\")\n",
        "convnext_study = optuna.create_study(direction='maximize')\n",
        "convnext_objective = create_objective('convnext_base')\n",
        "convnext_study.optimize(convnext_objective, n_trials=10)\n",
        "convnext_best_params = convnext_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wmusbfUPX1y-",
        "outputId": "c3c79e91-4fae-4253-9bf1-1d0a5f7c6e58"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=convnext_best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=convnext_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "convnext_model = timm.create_model(\n",
        "    \"convnext_base\",\n",
        "    pretrained=True,\n",
        "    num_classes=len(class_names),\n",
        "    drop_rate=convnext_best_params['dropout_rate'],\n",
        "    drop_path_rate=convnext_best_params['dropout_rate']/2\n",
        ")\n",
        "\n",
        "convnext_model, con_train_acc_history, con_val_acc_history, con_time, con_val_acc = train_model(convnext_model, train_loader, val_loader, convnext_best_params, num_epochs=10)\n",
        "\n",
        "# Evaluate\n",
        "y_true, y_pred, con_test_acc = evaluate_model(convnext_model, test_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for ConvNext:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, \"ConvNext\")\n",
        "\n",
        "print(f\"Training Time for ConvNext: {con_time:.2f} seconds\\n\")\n",
        "print(f\"Accuracy for ConvNext: {con_test_acc:.2f}\\n\")\n",
        "\n",
        "# Free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjr7DHOBINd2"
      },
      "source": [
        "The ConvNext model shows strong performance, achieving a test accuracy of 87.39% but the training time seems to be the longest out of the 4 models with 757.44 seconds.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The model achieved perfect precision and recall for: Florist, Greenhouse, Cloister, Gameroom, Studiomusic.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Top 3 best performing categories:\n",
        "1. Pantry\n",
        "2. Pool Inside\n",
        "3. Prison Cell\n",
        "\n",
        "\n",
        "---\n",
        "Top 3 worst performig categories:\n",
        "1. Toystore\n",
        "2. Museum\n",
        "3. Jewellery Shop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGdKKNL3koiq"
      },
      "source": [
        "## **Step 7: EfficientNet Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFlxC5ZE9di"
      },
      "source": [
        "Third model is EfficientNet-B0 -- A lightweight CNN that uses Neural Architecture Search (NAS) to optimize depth, width and resolution scaling, achieving high accuracy with minimal computational costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "6d28f62f6b5841199c67cbcfa29ac5e4",
            "97fff0161d7543a7a884754166344aab",
            "3632d7dbe5a44ec08c3be81a3209ae2c",
            "498aa5768e224787ac9efb2b9cd62fb6",
            "7c23e917d5e343678cee4a037457e5a6",
            "f8cd4aadecac4f3bacf955932e1a755b",
            "d78fc94f37cd49cc92bd399322383474",
            "6972b82f266a4bb0b9f98ba0b5711bee",
            "0a2b1f90bc2a4ce6ba941cb3df656372",
            "289879d68abf478a803250e113f2ef4e",
            "232d8af088714eb8b5ad04fc7afbc62d"
          ]
        },
        "id": "tYcHdLL_bM_0",
        "outputId": "b3ef1a81-7137-4d0c-aad5-fbd94e3e7037"
      },
      "outputs": [],
      "source": [
        "# For EfficientNet\n",
        "print(\"Optimizing EfficientNet hyperparameters...\")\n",
        "efficientnet_study = optuna.create_study(direction='maximize')\n",
        "efficientnet_objective = create_objective('efficientnet_b0')\n",
        "efficientnet_study.optimize(efficientnet_objective, n_trials=10)\n",
        "efficientnet_best_params = efficientnet_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zjz_f7hZX3Cx",
        "outputId": "4932f923-d9f1-42de-8045-8a4a84baf237"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=efficientnet_best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=efficientnet_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "efficientnet_model = timm.create_model(\n",
        "    \"efficientnet_b0\",\n",
        "    pretrained=True,\n",
        "    num_classes=len(class_names),\n",
        "    drop_rate=efficientnet_best_params['dropout_rate'],\n",
        "    drop_path_rate=efficientnet_best_params['dropout_rate']/2\n",
        ")\n",
        "\n",
        "efficientnet_model, eff_train_acc_history, eff_val_acc_history, eff_time, eff_val_acc = train_model(efficientnet_model, train_loader, val_loader, efficientnet_best_params, num_epochs=10)\n",
        "\n",
        "# Evaluate\n",
        "y_true, y_pred, eff_test_acc = evaluate_model(efficientnet_model, test_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for EfficientNet:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, \"EfficientNet\")\n",
        "\n",
        "print(f\"Training Time for EfficientNet: {eff_time:.2f} seconds\\n\")\n",
        "print(f\"Accuracy for EfficientNet: {eff_test_acc:.2f}\\n\")\n",
        "\n",
        "# Free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-hK5YskInoH"
      },
      "source": [
        "Overall, the EfficientNetB0 model achieved an accuracy of 67.91% with the fastest training time at 345.81 seconds which suggest that it may not be well-suited for this dataset unless further fine-tuning is done.\n",
        "\n",
        "\n",
        "---\n",
        "Top 3 best performing categories:\n",
        "1. Cloister\n",
        "2. Bowling\n",
        "3. Casino\n",
        "\n",
        "\n",
        "---\n",
        "Top 3 worst performing categories:\n",
        "1. Shoeshop\n",
        "2. Toystore\n",
        "3. Jewellery Shop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV9zgXPdmLRU"
      },
      "source": [
        "## **Step 8: SwinSmall Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUSCGEHkFSMJ"
      },
      "source": [
        "Last model is Swin Transformer (Swin-Small) -- A hierarchical Vision Transformer that introduces shifted window attention, improving efficiency and scalability for high-resolution images while preserving local and global context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "b48800e90ad5408dadbb4f9fbbb72881",
            "cc8ee3e6c6084e4381f6e301613e6c16",
            "998565bbbd5545bcae74ae5320e7f5be",
            "2318ee1eb99343bb882e3a55d1a1c2df",
            "a3228278e8424269847905fbb1f12bbe",
            "0fd40a320ba74746a9f183bbe6e169a7",
            "f27e9cd01b0546c3a18d6c931b47ac47",
            "8be51cce56b2482abbe6536b7ca12bf3",
            "bd8efacd02614e96b050c32a6bfa6344",
            "5e803f589173494090499ede682f8047",
            "d7fcca51f1e94f5fa2e5f0e6d414d6f1"
          ]
        },
        "id": "cbcwc3LjbM_1",
        "outputId": "eb29dc0f-b8d6-437e-b3a1-3dcf1520ecbe"
      },
      "outputs": [],
      "source": [
        "# For Swin\n",
        "print(\"Optimizing Swin Transformer hyperparameters...\")\n",
        "swin_study = optuna.create_study(direction='maximize')\n",
        "swin_objective = create_objective('swin_small_patch4_window7_224')\n",
        "swin_study.optimize(swin_objective, n_trials=10)\n",
        "swin_best_params = swin_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KJfhh11N0-TJ",
        "outputId": "0d04d167-d59c-4b0a-e561-d4f3bd5ed316"
      },
      "outputs": [],
      "source": [
        "# Create data loaders with best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=swin_best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=swin_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "swin_model = timm.create_model(\n",
        "    \"swin_small_patch4_window7_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=len(class_names),\n",
        "    drop_rate=swin_best_params['dropout_rate'],\n",
        "    drop_path_rate=swin_best_params['dropout_rate']/2\n",
        ")\n",
        "\n",
        "swin_model, swin_train_acc_history, swin_val_acc_history, swin_time, swin_val_acc = train_model(swin_model, train_loader, val_loader, swin_best_params, num_epochs=10)\n",
        "\n",
        "# Evaluate\n",
        "y_true, y_pred, swin_test_acc = evaluate_model(swin_model, test_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Swin-Small:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, \"Swin-Small\")\n",
        "\n",
        "print(f\"Training Time for Swin-Small: {swin_time:.2f} seconds\\n\")\n",
        "print(f\"Accuracy for Swin-Small: {swin_test_acc:.4%}\\n\")\n",
        "\n",
        "# Free up memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yneRUDUIJUCw"
      },
      "source": [
        "The Swin-Small model achieved 85.44% accuracy with a training time of 614.23 seconds, which is a good balance between ConvNext and ViT in terms of both training time and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "Top 3 best performing categories:\n",
        "1. Bowling\n",
        "2. Computer Room\n",
        "3. Florist\n",
        "\n",
        "\n",
        "---\n",
        "Top 3 worst performing categories:\n",
        "1. Auditorium\n",
        "2. Children's Room\n",
        "3. Lobby\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBcvwgxKyn0t"
      },
      "source": [
        "## **Step 9: Store Training History**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YG23wOJq5p"
      },
      "source": [
        "Here we organize the training history for the 4 models into a dictionary, storing their training accuracy, validation accuracy, training time and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS1U1RC80PcD"
      },
      "outputs": [],
      "source": [
        "training_history = {\n",
        "    \"Vision Transformer\": {\n",
        "        \"train_acc\": vit_train_acc_history,\n",
        "        \"val_acc\": vit_val_acc_history,\n",
        "        \"training_time\": vit_time,\n",
        "        \"accuracy\": vit_test_acc\n",
        "    },\n",
        "    \"ConvNext\": {\n",
        "        \"train_acc\": con_train_acc_history,\n",
        "        \"val_acc\": con_val_acc_history,\n",
        "        \"training_time\": con_time,\n",
        "        \"accuracy\": con_test_acc\n",
        "    },\n",
        "    \"EfficientNet\": {\n",
        "        \"train_acc\": eff_train_acc_history,\n",
        "        \"val_acc\": eff_val_acc_history,\n",
        "        \"training_time\": eff_time,\n",
        "        \"accuracy\": eff_test_acc\n",
        "    },\n",
        "    \"Swin-Small\": {\n",
        "        \"train_acc\": swin_train_acc_history,\n",
        "        \"val_acc\": swin_val_acc_history,\n",
        "        \"training_time\": swin_time,\n",
        "        \"accuracy\": swin_test_acc\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftODXLu_07gC"
      },
      "source": [
        "## **Step 10: Plot Training and Validation History for all models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "NduwJNcS1GjT",
        "outputId": "e7996fa7-dcd5-4ab5-dc6b-9287180e4203"
      },
      "outputs": [],
      "source": [
        "# Plot Training & Validation Accuracy for All Models\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "colors = ['b', 'g', 'r', 'm']\n",
        "\n",
        "for i, (model_name, history) in enumerate(training_history.items()):\n",
        "    train_acc = history[\"train_acc\"]\n",
        "    val_acc = history[\"val_acc\"]\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, train_acc, color=colors[i], marker=\"o\", label=f\"{model_name} - Train\")\n",
        "    plt.plot(epochs, val_acc, color=colors[i], linestyle=\"dashed\", marker=\"o\", label=f\"{model_name} - Val\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training & Validation Accuracy for All Models\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print Best Model\n",
        "best_model = max(training_history, key=lambda x: training_history[x][\"accuracy\"])\n",
        "best_acc = training_history[best_model][\"accuracy\"]\n",
        "print(f\"\\nBest Model: {best_model} with Test Accuracy: {best_acc:.4f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-5Ss-C0KZHT"
      },
      "source": [
        "The graph titled \"Training & Validation Accuracy for All Models\" compares the performance of several models during training and validation across different epochs. The models included are Vision Transformer, ConvNext, EfficientNet, and Swin-Small. The epochs range from 0 to 10.\n",
        "\n",
        "- **Vision Transformer**: Both training and validation accuracy increase steadily with each epoch, indicating that the model is learning effectively without overfitting.\n",
        "\n",
        "- **ConvNext**: This model shows a significant increase in both training and validation accuracy, reaching the highest test accuracy of 87.1642%. This suggests that ConvNext is the best-performing model among those evaluated.\n",
        "\n",
        "- **EfficientNet**: The training and validation accuracy for EfficientNet also improve with each epoch, but the rate of improvement is slower compared to ConvNext.\n",
        "\n",
        "- **Swin-Small**: Similar to the other models, Swin-Small's accuracy increases with each epoch, but it does not surpass the performance of ConvNext.\n",
        "\n",
        "Overall, ConvNext stands out as the best model with the highest test accuracy, indicating it generalizes well to unseen data. The consistent increase in accuracy across epochs for all models suggests that they are learning effectively, with ConvNext leading in performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFCnbxOfowcI"
      },
      "source": [
        "## **Step 11: Training ConvNext on full dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LcW0InhKxrH"
      },
      "source": [
        "Lastly, we train and evaluate the best model which is ConvNext on the full dataset now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VFzTTBOQ02-O",
        "outputId": "f9594593-a91d-4a00-ab07-0c33a464ed19"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder(root=\"full_dataset/train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=\"full_dataset/val\", transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=\"full_dataset/test\", transform=eval_transform)\n",
        "\n",
        "# Create data loaders with best batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=convnext_best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=convnext_best_params['batch_size'], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=convnext_best_params['batch_size'], shuffle=False)\n",
        "\n",
        "# For ConvNeXt\n",
        "print(\"Optimizing ConvNext hyperparameters...\")\n",
        "convnext_study = optuna.create_study(direction='maximize')\n",
        "convnext_objective = create_objective('convnext_base')\n",
        "convnext_study.optimize(convnext_objective, n_trials=10)\n",
        "convnext_best_params = convnext_study.best_params\n",
        "\n",
        "convnext_model = timm.create_model(\n",
        "    \"convnext_base\",\n",
        "    pretrained=True,\n",
        "    num_classes=len(class_names),\n",
        "    drop_rate=convnext_best_params['dropout_rate'],\n",
        "    drop_path_rate=convnext_best_params['dropout_rate']/2\n",
        ")\n",
        "\n",
        "convnext_model, con_train_acc_history, con_val_acc_history, con_time, con_val_acc = train_model(convnext_model, train_loader, val_loader, convnext_best_params, num_epochs=10)\n",
        "\n",
        "# Evaluate\n",
        "y_true, y_pred, con_test_acc = evaluate_model(convnext_model, test_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for ConvNext:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, \"ConvNext\")\n",
        "\n",
        "print(f\"Training Time for ConvNext: {con_time:.2f} seconds\\n\")\n",
        "print(f\"Accuracy for ConvNext: {con_test_acc:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_W5ykHCLZDT"
      },
      "source": [
        "This ConvNext model performed best with a final test accuracy of 89.72%.\n",
        "\n",
        "\n",
        "---\n",
        "The best trial ahieved 89.09% validation accuracy with:\n",
        "1. Learning Rate: 2.09e-5\n",
        "2. Weight Decay: 0.000118\n",
        "3. Dropout Rate: 0.236\n",
        "4. Batch Size: 16\n",
        "\n",
        "\n",
        "---\n",
        "Key Observations:\n",
        "1. Early stopping was triggered, meaning the model stopped training once validation performance stopped improving.\n",
        "2. Final training accuracy was 96.47%, with validation accuracy stabilizing around 88.73% before testing at 89.72%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M9C1dnOGzi1"
      },
      "outputs": [],
      "source": [
        "torch.save(convnext_model.state_dict(), 'convnext_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "YdxNiyzxV-YU",
        "outputId": "b2e2630a-c7e6-4d7c-ece0-09243e260e22"
      },
      "outputs": [],
      "source": [
        "training_history = {\n",
        "    \"ConvNext\": {\n",
        "        \"train_acc\": con_train_acc_history,\n",
        "        \"val_acc\": con_val_acc_history,\n",
        "        \"training_time\": con_time,\n",
        "        \"accuracy\": con_test_acc\n",
        "    }\n",
        "}\n",
        "\n",
        "# Plot Training & Validation Accuracy for All Models\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "colors = ['g']\n",
        "\n",
        "for i, (model_name, history) in enumerate(training_history.items()):\n",
        "    train_acc = history[\"train_acc\"]\n",
        "    val_acc = history[\"val_acc\"]\n",
        "    epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, train_acc, color=colors[i], marker=\"o\", label=f\"{model_name} - Train\")\n",
        "    plt.plot(epochs, val_acc, color=colors[i], linestyle=\"dashed\", marker=\"o\", label=f\"{model_name} - Val\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training & Validation Accuracy for All Models\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPaqqXPhFsk2"
      },
      "source": [
        "### **Step 12: Inference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cEzXZ1bErM"
      },
      "source": [
        "To test our model on real world examples, we use our saved model to run inference on a set of our own images. Our function code processes each image, then outputs the top 3 predictions inferred from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VDmhbkQTFrgm",
        "outputId": "3f611695-3d07-4cf0-f657-0176aa2f36c9"
      },
      "outputs": [],
      "source": [
        "def predict_scene(model, image_path, class_names):\n",
        "    # Load and preprocess the image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Move to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Get probabilities\n",
        "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "        # Get top 3 predictions\n",
        "        top3_prob, top3_indices = torch.topk(probabilities, 3)\n",
        "\n",
        "    # Return results\n",
        "    results = []\n",
        "    for prob, idx in zip(top3_prob, top3_indices):\n",
        "        results.append({\n",
        "            'class': class_names[idx],\n",
        "            'probability': prob.item() * 100\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "def test_custom_images(model_path, image_folder):\n",
        "    # Load the saved model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = timm.create_model(\"convnext_base\", pretrained=False, num_classes=len(class_names))\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Process each image\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        print(f\"\\nPredicting scene for {image_file}:\")\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = predict_scene(model, image_path, class_names)\n",
        "\n",
        "        # Display results\n",
        "        for i, pred in enumerate(predictions, 1):\n",
        "            print(f\"{i}. {pred['class']}: {pred['probability']:.2f}%\")\n",
        "\n",
        "        # Display image\n",
        "        img = Image.open(image_path)\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Top prediction: {predictions[0]['class']}\")\n",
        "        plt.show()\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure to import required libraries if not already imported\n",
        "    from PIL import Image\n",
        "    import os\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Specify paths\n",
        "    MODEL_PATH = 'convnext_model.pth'  # Path to your saved model\n",
        "    IMAGE_FOLDER = 'test_images'  # Folder containing your test images\n",
        "\n",
        "    # Create test_images folder if it doesn't exist\n",
        "    os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
        "\n",
        "    print(\"Place your images in the 'test_images' folder and run this script.\")\n",
        "    print(\"Supported formats: .jpg, .jpeg, .png\")\n",
        "\n",
        "    # Test the model on your images\n",
        "    test_custom_images(MODEL_PATH, IMAGE_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpJKZqKsb2hM"
      },
      "source": [
        "The model is able to correctly classify all 4 images that were given to it, with at least 95% confidence for each prediction."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "nbformat": 4,
    "nbformat_minor": 0
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
